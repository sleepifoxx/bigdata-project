version: "3.8"

services:
  # ================== Zookeeper & Kafka ==================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.6.1
    container_name: kafka
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper

  # ================== Redis ==================
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"

  # ================== Hadoop Cluster ==================
  namenode:
    build: ./namenode
    container_name: namenode
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env
    ports:
      - "9870:9870"

  resourcemanager:
    build: ./resourcemanager
    container_name: resourcemanager
    restart: on-failure
    depends_on:
      - namenode
      - datanode1
      - datanode2
    env_file:
      - ./hadoop.env
    ports:
      - "8089:8088"

  historyserver:
    build: ./historyserver
    container_name: historyserver
    depends_on:
      - namenode
      - datanode1
      - datanode2
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env
    ports:
      - "8188:8188"

  nodemanager1:
    build: ./nodemanager
    container_name: nodemanager1
    depends_on:
      - namenode
      - datanode1
      - datanode2
    env_file:
      - ./hadoop.env
    ports:
      - "8042:8042"

  datanode1:
    build: ./datanode
    container_name: datanode1
    restart: always
    depends_on:
      - namenode
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data
    env_file:
      - ./hadoop.env

  datanode2:
    build: ./datanode
    container_name: datanode2
    restart: always
    depends_on:
      - namenode
    volumes:
      - hadoop_datanode2:/hadoop/dfs/data
    env_file:
      - ./hadoop.env

  # ================== PySpark Notebook ==================
  pyspark:
    image: jupyter/pyspark-notebook:spark-3.3.0
    container_name: pyspark-notebook
    ports:
      - "8888:8888"
    volumes:
      - ./notebook/:/home/jovyan/work/
    depends_on:
      - namenode
      - datanode1
      - datanode2
      - resourcemanager
      - historyserver
      - nodemanager1
      - spark-master
      - spark-worker
      - kafka
      - zookeeper
      - redis
    command: >
      bash -c "
        cd /home/jovyan/work &&
        chmod +x data/get_data.sh &&
        chmod +x run_all.sh &&
        ./data/get_data.sh &&
        pip install -r requirements.txt &&
        python upload_to_hdfs.py &&
        ./run_all.sh &&
        start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''
      "

  # ================== Spark Cluster ==================
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    ports:
      - "8080:8080"   # Spark Master UI
      - "7077:7077"   # Spark Master port
    env_file:
      - ./hadoop.env

  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - "8081:8081"  # Spark Worker UI
    env_file:
      - ./hadoop.env

volumes:
  hadoop_namenode:
  hadoop_datanode1:
  hadoop_datanode2:
  hadoop_historyserver:
