#!/usr/bin/env python3
"""
Script ki·ªÉm tra model ƒë√£ ƒë∆∞·ª£c train ch∆∞a v√† test nhanh prediction
"""

import os
import sys

def check_models():
    """Ki·ªÉm tra models ƒë√£ ƒë∆∞·ª£c train"""
    print("="*80)
    print("üîç CHECKING TRAINED MODELS")
    print("="*80)
    print()
    
    # Check GBT model
    gbt_path = "/user/jovyan/gbt_fraud_model"
    if os.path.exists(gbt_path):
        files = os.listdir(gbt_path)
        print(f"‚úÖ GBT Model found: {gbt_path}/")
        print(f"   Files: {len(files)} files")
        print(f"   Size: {sum(os.path.getsize(os.path.join(gbt_path, f)) for f in files if os.path.isfile(os.path.join(gbt_path, f)))/1024/1024:.2f} MB")
    else:
        print(f"‚ùå GBT Model NOT found: {gbt_path}/")
        print("   üëâ Ch·∫°y train.ipynb ƒë·ªÉ train model!")
        return False
    
    print()
    
    # Check Scaler model
    scaler_path = "spark_scaler_model"
    if os.path.exists(scaler_path):
        files = os.listdir(scaler_path)
        print(f"‚úÖ Scaler Model found: {scaler_path}/")
        print(f"   Files: {len(files)} files")
        print(f"   Size: {sum(os.path.getsize(os.path.join(scaler_path, f)) for f in files if os.path.isfile(os.path.join(scaler_path, f)))/1024/1024:.2f} MB")
    else:
        print(f"‚ùå Scaler Model NOT found: {scaler_path}/")
        print("   üëâ Ch·∫°y train.ipynb ƒë·ªÉ train model!")
        return False
    
    print()
    print("="*80)
    print("‚úÖ T·∫§T C·∫¢ MODELS ƒê√É S·∫¥N S√ÄNG!")
    print("="*80)
    return True


def test_prediction():
    """Test quick prediction v·ªõi sample data"""
    print()
    print("="*80)
    print("üß™ TESTING PREDICTION")
    print("="*80)
    print()
    
    try:
        from pyspark.sql import SparkSession
        from pyspark.ml.classification import GBTClassificationModel
        from pyspark.ml.feature import VectorAssembler, StandardScalerModel
        from pyspark.sql.types import *
        
        # T·∫°o Spark session
        print("üîß Kh·ªüi t·∫°o Spark Session...")
        spark = SparkSession.builder \
            .appName("Model Test") \
            .config("spark.driver.memory", "2g") \
            .getOrCreate()
        
        spark.sparkContext.setLogLevel("ERROR")
        
        # Load models
        print("üì¶ Loading models...")
        gbt_model = GBTClassificationModel.load("gbt_fraud_model")
        scaler_model = StandardScalerModel.load("spark_scaler_model")
        print("‚úÖ Models loaded successfully!")
        
        # T·∫°o sample data (1 transaction)
        print()
        print("üìä Creating sample transaction...")
        
        schema = StructType([
            StructField("type_encoded", IntegerType(), False),
            StructField("amount_log", DoubleType(), False),
            StructField("errorBalanceOrig", DoubleType(), False),
            StructField("errorBalanceDest", DoubleType(), False),
            StructField("amount_over_oldbalance", DoubleType(), False),
            StructField("hour", IntegerType(), False)
        ])
        
        # Sample: Suspicious transaction (high amount, errors)
        sample_data = [(1, 12.5, 5000.0, -5000.0, 2.5, 3)]
        df = spark.createDataFrame(sample_data, schema)
        
        print("   Type: TRANSFER (encoded=1)")
        print("   Amount: ~$270,000 (log=12.5)")
        print("   Hour: 3 AM (suspicious)")
        
        # Create features vector
        feature_cols = ['type_encoded', 'amount_log', 'errorBalanceOrig', 
                       'errorBalanceDest', 'amount_over_oldbalance', 'hour']
        
        assembler = VectorAssembler(inputCols=feature_cols, outputCol="features_raw")
        df_assembled = assembler.transform(df)
        
        # Scale
        df_scaled = scaler_model.transform(df_assembled)
        
        # Predict
        print()
        print("üéØ Making prediction...")
        predictions = gbt_model.transform(df_scaled)
        
        # Get result
        result = predictions.select("prediction", "probability").collect()[0]
        pred = int(result.prediction)
        prob = result.probability[1]
        
        print()
        print("="*80)
        print("üìä PREDICTION RESULT")
        print("="*80)
        print(f"Prediction: {'üö® FRAUD' if pred == 1 else '‚úÖ NORMAL'}")
        print(f"Fraud Probability: {prob:.4f} ({prob*100:.2f}%)")
        print("="*80)
        
        spark.stop()
        return True
        
    except ImportError as e:
        print(f"‚ùå L·ªói import: {e}")
        print("   ƒê·∫£m b·∫£o PySpark ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t")
        return False
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    # Check models exist
    if not check_models():
        print()
        print("‚ö†Ô∏è  C·∫ßn train model tr∆∞·ªõc!")
        print("   üëâ M·ªü v√† ch·∫°y notebook: train.ipynb")
        sys.exit(1)
    
    # Test prediction
    print()
    choice = input("B·∫°n c√≥ mu·ªën test prediction kh√¥ng? (y/n): ")
    if choice.lower() == 'y':
        test_prediction()
    
    print()
    print("‚úÖ Ki·ªÉm tra ho√†n t·∫•t!")
    print()


if __name__ == "__main__":
    main()
