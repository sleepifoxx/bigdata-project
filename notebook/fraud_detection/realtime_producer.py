"""
Kafka Producer - Fraud Detection Real-time Simulation
ƒê·ªçc d·ªØ li·ªáu t·ª´ HDFS v√† g·ª≠i 5 giao d·ªãch/gi√¢y v√†o Kafka
"""

import json
import time
import logging
from kafka import KafkaProducer
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import sys

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# C·∫•u h√¨nh
KAFKA_BOOTSTRAP_SERVERS = ['kafka:9092']
KAFKA_TOPIC = 'fraud_transactions'
HDFS_INPUT_PATH = 'hdfs://namenode:9000/data/input/paysim_realtime.csv'
TRANSACTIONS_PER_BATCH = 100
BATCH_INTERVAL = 10.0  # gi√¢y
TRANSACTIONS_PER_SECOND = 10  # Speed within each batch


class FraudDataProducer:
    """Producer ƒë·ªÉ gi·∫£ l·∫≠p d·ªØ li·ªáu giao d·ªãch real-time"""
    
    def __init__(self, bootstrap_servers, topic, hdfs_path):
        self.topic = topic
        self.hdfs_path = hdfs_path
        
        # Kh·ªüi t·∫°o Kafka Producer
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            key_serializer=lambda k: str(k).encode('utf-8') if k else None,
            acks='all',
            retries=3,
            compression_type='gzip'
        )
        
        # Kh·ªüi t·∫°o Spark Session
        self.spark = SparkSession.builder \
            .appName("FraudDataProducer") \
            .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
            .getOrCreate()
        
        self.spark.sparkContext.setLogLevel("WARN")
        
        logger.info(f"Kafka Producer initialized. Topic: {topic}")
        logger.info(f"HDFS Path: {hdfs_path}")
    
    def load_data(self):
        """ƒê·ªçc d·ªØ li·ªáu t·ª´ HDFS"""
        try:
            logger.info(f"ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ HDFS: {self.hdfs_path}")
            
            # ƒê·ªçc CSV t·ª´ HDFS
            df = self.spark.read.csv(
                self.hdfs_path,
                header=True,
                inferSchema=True
            )
            
            total_count = df.count()
            logger.info(f"ƒê√£ ƒë·ªçc {total_count:,} giao d·ªãch t·ª´ HDFS")
            
            # Chuy·ªÉn sang Pandas ƒë·ªÉ x·ª≠ l√Ω tu·∫ßn t·ª±
            # N·∫øu file qu√° l·ªõn, c√≥ th·ªÉ l·∫•y m·∫´u
            if total_count > 100000:
                logger.warning(f"File c√≥ {total_count:,} records, s·∫Ω l·∫•y 100,000 records ƒë·∫ßu")
                df = df.limit(100000)
            
            pdf = df.toPandas()
            logger.info(f"ƒê√£ chuy·ªÉn {len(pdf):,} giao d·ªãch sang Pandas")
            
            return pdf
            
        except Exception as e:
            logger.error(f"L·ªói khi ƒë·ªçc d·ªØ li·ªáu t·ª´ HDFS: {str(e)}")
            sys.exit(1)
    
    def send_transaction(self, transaction, index):
        """G·ª≠i m·ªôt giao d·ªãch ƒë·∫øn Kafka"""
        try:
            # Chuy·ªÉn ƒë·ªïi transaction th√†nh dictionary
            tx_dict = transaction.to_dict()
            
            # X·ª≠ l√Ω NaN values
            for key, value in tx_dict.items():
                if value != value:  # Check NaN
                    tx_dict[key] = None
                elif isinstance(value, (float, int)):
                    tx_dict[key] = float(value)
            
            # Th√™m metadata
            message = {
                'transaction_id': index,
                'timestamp': time.time(),
                'data': tx_dict
            }
            
            # G·ª≠i ƒë·∫øn Kafka
            key = f"tx_{index}"
            future = self.producer.send(
                self.topic,
                key=key,
                value=message
            )
            
            # Ch·ªù x√°c nh·∫≠n (non-blocking)
            record_metadata = future.get(timeout=10)
            
            # Log m·ªói 100 transactions
            if index % 100 == 0:
                logger.info(
                    f"Sent TX #{index} -> "
                    f"Partition: {record_metadata.partition}, "
                    f"Offset: {record_metadata.offset}"
                )
            
            return True
            
        except Exception as e:
            logger.error(f"L·ªói khi g·ª≠i transaction #{index}: {str(e)}")
            return False
    
    def run(self, transactions_per_batch=100, batch_interval=10.0):
        """Ch·∫°y producer v·ªõi batch processing"""
        logger.info("="*80)
        logger.info("FRAUD DETECTION BATCH PRODUCER")
        logger.info("="*80)
        
        # Load d·ªØ li·ªáu
        df = self.load_data()
        total_transactions = len(df)
        
        logger.info(f"Batch size: {transactions_per_batch} giao d·ªãch/batch")
        logger.info(f"Batch interval: {batch_interval} gi√¢y")
        logger.info(f"T·ªïng s·ªë giao d·ªãch: {total_transactions:,}")
        logger.info(f"S·ªë batch ∆∞·ªõc t√≠nh: {total_transactions//transactions_per_batch + 1}")
        logger.info("="*80)
        logger.info("B·∫Øt ƒë·∫ßu g·ª≠i batch... (Ctrl+C ƒë·ªÉ d·ª´ng)")
        logger.info("="*80)
        
        try:
            sent_count = 0
            batch_count = 0
            
            for i in range(0, total_transactions, transactions_per_batch):
                batch_start = time.time()
                batch_count += 1
                
                # L·∫•y batch giao d·ªãch
                batch_end = min(i + transactions_per_batch, total_transactions)
                batch = df.iloc[i:batch_end]
                current_batch_size = len(batch)
                
                logger.info(f"üöÄ Sending Batch #{batch_count} ({current_batch_size} transactions)...")
                
                # G·ª≠i t·ª´ng giao d·ªãch trong batch nhanh ch√≥ng
                batch_sent = 0
                for idx, row in batch.iterrows():
                    if self.send_transaction(row, idx):
                        sent_count += 1
                        batch_sent += 1
                
                # Flush sau m·ªói batch
                self.producer.flush()
                
                # T√≠nh th·ªùi gian ƒë√£ x·ª≠ l√Ω
                elapsed = time.time() - batch_start
                
                # Hi·ªÉn th·ªã progress
                progress = (batch_end / total_transactions) * 100
                logger.info(
                    f"‚úÖ Batch #{batch_count} completed: {batch_sent}/{current_batch_size} sent - "
                    f"Total: {sent_count:,}/{total_transactions:,} ({progress:.1f}%) - "
                    f"Batch time: {elapsed:.3f}s"
                )
                
                # Sleep ƒë·ªÉ ƒë·∫°t interval mong mu·ªën (tr·ª´ ƒëi th·ªùi gian x·ª≠ l√Ω)
                if i + transactions_per_batch < total_transactions:  # Kh√¥ng sleep ·ªü batch cu·ªëi
                    sleep_time = max(0, batch_interval - elapsed)
                    if sleep_time > 0:
                        logger.info(f"‚è≥ Waiting {sleep_time:.1f}s for next batch...")
                        time.sleep(sleep_time)
            
            # Flush cu·ªëi c√πng
            self.producer.flush()
            
            logger.info("="*80)
            logger.info(f"‚úÖ HO√ÄN TH√ÄNH!")
            logger.info(f"   T·ªïng giao d·ªãch ƒë√£ g·ª≠i: {sent_count:,}")
            logger.info(f"   T·ªïng th·ªùi gian: {time.time():.1f}s")
            logger.info("="*80)
            
        except KeyboardInterrupt:
            logger.info("\n‚ö†Ô∏è  ƒê√£ d·ª´ng producer b·ªüi ng∆∞·ªùi d√πng")
            logger.info(f"   ƒê√£ g·ª≠i: {sent_count:,} giao d·ªãch")
        except Exception as e:
            logger.error(f"L·ªói trong qu√° tr√¨nh g·ª≠i: {str(e)}")
        finally:
            self.close()
    
    def close(self):
        """ƒê√≥ng k·∫øt n·ªëi"""
        if self.producer:
            self.producer.flush()
            self.producer.close()
            logger.info("ƒê√£ ƒë√≥ng Kafka producer")
        
        if self.spark:
            self.spark.stop()
            logger.info("ƒê√£ d·ª´ng Spark session")


def main():
    """H√†m ch√≠nh"""
    producer = FraudDataProducer(
        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
        topic=KAFKA_TOPIC,
        hdfs_path=HDFS_INPUT_PATH
    )
    
    producer.run(transactions_per_batch=TRANSACTIONS_PER_BATCH, batch_interval=BATCH_INTERVAL)


if __name__ == "__main__":
    main()
