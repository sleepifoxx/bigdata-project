# Fraud Detection Real-time System

H·ªá th·ªëng ph√°t hi·ªán gian l·∫≠n giao d·ªãch theo th·ªùi gian th·ª±c s·ª≠ d·ª•ng Kafka, Spark Streaming v√† Machine Learning.

## Ki·∫øn tr√∫c

```
HDFS (paysim_realtime.csv)
    ‚Üì
Kafka Producer (5 tx/s)
    ‚Üì
Kafka Topic: fraud_transactions
    ‚Üì
Spark Streaming Consumer
    ‚Üì (ML Model)
Fraud Detection Results
```

## Quy tr√¨nh

### 1. Training Model (ƒê√£ ho√†n th√†nh)

S·ª≠ d·ª•ng notebook `train.ipynb` ƒë·ªÉ train GBT model:
- **Model**: GBTClassifier (Gradient Boosted Trees)
- **Features**: 6 features
  - `type_encoded`: Lo·∫°i giao d·ªãch (0: CASH_OUT, 1: TRANSFER)
  - `amount_log`: Log(amount + 1)
  - `errorBalanceOrig`: Sai s·ªë c√¢n b·∫±ng t√†i kho·∫£n ngu·ªìn
  - `errorBalanceDest`: Sai s·ªë c√¢n b·∫±ng t√†i kho·∫£n ƒë√≠ch
  - `amount_over_oldbalance`: T·ª∑ l·ªá amount/s·ªë d∆∞ c≈©
  - `hour`: Gi·ªù trong ng√†y (0-23)
- **Threshold t·ªëi ∆∞u**: 0.45 (c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh)
- **Output**: 
  - `gbt_fraud_model/`: Trained GBT model
  - `spark_scaler_model/`: StandardScaler model

### 2. Chu·∫©n b·ªã d·ªØ li·ªáu Real-time

Upload file d·ªØ li·ªáu l√™n HDFS:

```bash
# T·∫£i d·ªØ li·ªáu m·∫´u (n·∫øu ch∆∞a c√≥)
wget https://example.com/paysim_realtime.csv

# Upload l√™n HDFS
docker exec -it namenode hdfs dfs -mkdir -p /data/input
docker exec -it namenode hdfs dfs -put paysim_realtime.csv /data/input/

# Ki·ªÉm tra
docker exec -it namenode hdfs dfs -ls /data/input/
```

### 3. Ch·∫°y Kafka Producer

Producer ƒë·ªçc d·ªØ li·ªáu t·ª´ HDFS v√† g·ª≠i 5 giao d·ªãch/gi√¢y v√†o Kafka:

```bash
cd /home/jovyan/work/fraud_detection
python realtime_producer.py
```

**Output m·∫´u:**
```
2025-11-15 10:00:00 - INFO - FRAUD DETECTION REAL-TIME PRODUCER
2025-11-15 10:00:00 - INFO - Kafka Producer initialized. Topic: fraud_transactions
2025-11-15 10:00:01 - INFO - ƒê√£ ƒë·ªçc 50,000 giao d·ªãch t·ª´ HDFS
2025-11-15 10:00:01 - INFO - T·ªëc ƒë·ªô: 5 giao d·ªãch/gi√¢y
2025-11-15 10:00:01 - INFO - B·∫Øt ƒë·∫ßu g·ª≠i d·ªØ li·ªáu...
2025-11-15 10:00:02 - INFO - Progress: 5/50,000 (0.0%) - Sent: 5
2025-11-15 10:00:03 - INFO - Progress: 10/50,000 (0.0%) - Sent: 10
...
```

### 4. Ch·∫°y Spark Streaming Consumer

Consumer nh·∫≠n d·ªØ li·ªáu t·ª´ Kafka, x·ª≠ l√Ω v√† d·ª± ƒëo√°n gian l·∫≠n:

```bash
cd /home/jovyan/work/fraud_detection
python realtime_consumer.py
```

Ho·∫∑c ch·∫°y v·ªõi spark-submit:

```bash
spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 \
  realtime_consumer.py
```

**Output:**

#### Stream 1: All Transactions with Predictions
```
tx_id  transaction_time     tx_type   amount    from_account  to_account  actual_fraud  predicted_fraud  fraud_prob_pct
1234   2025-11-15 10:00:01  TRANSFER  150000.0  C123456       C789012     0             0                12.5
1235   2025-11-15 10:00:01  CASH_OUT  500000.0  C234567       C890123     1             1                87.3
```

#### Stream 2: Fraud Alerts Only
```
üö® FRAUD_ALERTS:
tx_id  transaction_time     tx_type   amount     from_account  to_account  fraud_prob_pct
1235   2025-11-15 10:00:01  CASH_OUT  500000.0   C234567       C890123     87.3
1298   2025-11-15 10:00:14  TRANSFER  2500000.0  C345678       C901234     92.1
```

#### Stream 3: Performance Metrics (Every 30s)
```
PERFORMANCE_METRICS:
window_start         window_end           total_tx  actual_frauds  predicted_frauds  accuracy_pct  true_pos  false_pos  false_neg  avg_fraud_prob_pct
2025-11-15 10:00:00  2025-11-15 10:00:30  150       8              9                 96.7          7         2          1          23.5
```

## C·∫•u h√¨nh

### realtime_producer.py

```python
KAFKA_BOOTSTRAP_SERVERS = ['kafka:9092']
KAFKA_TOPIC = 'fraud_transactions'
HDFS_INPUT_PATH = 'hdfs://namenode:9000/data/input/paysim_realtime.csv'
TRANSACTIONS_PER_SECOND = 5  # ƒêi·ªÅu ch·ªânh t·ªëc ƒë·ªô
```

### realtime_consumer.py

```python
KAFKA_BOOTSTRAP_SERVERS = "kafka:9092"
KAFKA_TOPIC = "fraud_transactions"
MODEL_PATH = "gbt_fraud_model"
SCALER_PATH = "spark_scaler_model"
FRAUD_THRESHOLD = 0.45  # ƒêi·ªÅu ch·ªânh threshold (0.0-1.0)
```

## C√°c Ch·ªâ S·ªë ƒê√°nh Gi√°

### Performance Metrics

- **Accuracy**: T·ª∑ l·ªá d·ª± ƒëo√°n ƒë√∫ng t·ªïng th·ªÉ
- **True Positives (TP)**: Ph√°t hi·ªán ƒë√∫ng gian l·∫≠n
- **False Positives (FP)**: C·∫£nh b√°o nh·∫ßm (kh√¥ng gian l·∫≠n nh∆∞ng b√°o gian l·∫≠n)
- **False Negatives (FN)**: B·ªè s√≥t gian l·∫≠n (gian l·∫≠n nh∆∞ng kh√¥ng ph√°t hi·ªán)
- **Precision**: TP / (TP + FP) - ƒê·ªô ch√≠nh x√°c c·ªßa c·∫£nh b√°o
- **Recall**: TP / (TP + FN) - T·ª∑ l·ªá ph√°t hi·ªán ƒë∆∞·ª£c gian l·∫≠n

### Threshold Tuning

ƒêi·ªÅu ch·ªânh `FRAUD_THRESHOLD` ƒë·ªÉ c√¢n b·∫±ng Precision/Recall:

- **Threshold cao (0.6-0.9)**: 
  - ‚úÖ Precision cao (√≠t false positive)
  - ‚ùå Recall th·∫•p (b·ªè s√≥t nhi·ªÅu gian l·∫≠n)
  - **Use case**: Chi ph√≠ false positive cao

- **Threshold th·∫•p (0.2-0.4)**: 
  - ‚úÖ Recall cao (ph√°t hi·ªán nhi·ªÅu gian l·∫≠n)
  - ‚ùå Precision th·∫•p (nhi·ªÅu false positive)
  - **Use case**: Chi ph√≠ b·ªè s√≥t gian l·∫≠n cao

- **Threshold t·ªëi ∆∞u (0.45)**: 
  - ‚öñÔ∏è C√¢n b·∫±ng Precision/Recall
  - **Use case**: ƒêa s·ªë tr∆∞·ªùng h·ª£p

## Features Engineering

H·ªá th·ªëng t·ª± ƒë·ªông t·∫°o c√°c features t·ª´ d·ªØ li·ªáu th√¥:

1. **type_encoded**: M√£ h√≥a lo·∫°i giao d·ªãch
2. **errorBalanceOrig**: `oldBalanceOrig - newBalanceOrig - amount`
   - Ph√°t hi·ªán b·∫•t th∆∞·ªùng v·ªÅ s·ªë d∆∞
3. **errorBalanceDest**: `oldBalanceDest + amount - newBalanceDest`
   - Ph√°t hi·ªán b·∫•t th∆∞·ªùng v·ªÅ s·ªë d∆∞ ƒë√≠ch
4. **amount_over_oldbalance**: `amount / oldBalanceOrig`
   - Ph√°t hi·ªán r√∫t ti·ªÅn v∆∞·ª£t s·ªë d∆∞
5. **hour**: `step % 24`
   - Ph√°t hi·ªán giao d·ªãch b·∫•t th∆∞·ªùng theo th·ªùi gian
6. **amount_log**: `log(amount + 1)`
   - X·ª≠ l√Ω ph√¢n ph·ªëi l·ªách c·ªßa amount

## Monitoring

### Kafka Topics

```bash
# Xem messages trong topic
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic fraud_transactions \
  --from-beginning \
  --max-messages 10
```

### Spark UI

- Spark Master: http://localhost:8080
- Spark Application UI: http://localhost:4040 (khi consumer ƒëang ch·∫°y)

### HDFS Web UI

- NameNode: http://localhost:9870

## Troubleshooting

### L·ªói: Model not found

```bash
# Ki·ªÉm tra model ƒë√£ ƒë∆∞·ª£c train ch∆∞a
ls -la gbt_fraud_model/
ls -la spark_scaler_model/

# N·∫øu ch∆∞a c√≥, ch·∫°y notebook train.ipynb
```

### L·ªói: HDFS file not found

```bash
# Ki·ªÉm tra file tr√™n HDFS
docker exec -it namenode hdfs dfs -ls /data/input/

# Upload file
docker exec -it namenode hdfs dfs -put paysim_realtime.csv /data/input/
```

### L·ªói: Kafka connection refused

```bash
# Ki·ªÉm tra Kafka ƒëang ch·∫°y
docker ps | grep kafka

# Restart Kafka
docker-compose restart kafka zookeeper
```

### L·ªói: Out of memory

```python
# Gi·∫£m t·ªëc ƒë·ªô trong producer
TRANSACTIONS_PER_SECOND = 2  # Gi·∫£m t·ª´ 5 xu·ªëng 2

# Ho·∫∑c l·∫•y m·∫´u d·ªØ li·ªáu nh·ªè h∆°n trong producer
if total_count > 50000:
    df = df.limit(50000)  # Gi·∫£m t·ª´ 100,000 xu·ªëng 50,000
```

## M·ªü r·ªông

### 1. L∆∞u k·∫øt qu·∫£ v√†o Database

Th√™m sink v√†o consumer:

```python
# V√≠ d·ª•: L∆∞u fraud alerts v√†o Parquet
fraud_alerts.writeStream \
    .format("parquet") \
    .option("path", "hdfs://namenode:9000/data/fraud_alerts") \
    .option("checkpointLocation", "/tmp/fraud_alerts_checkpoint") \
    .start()
```

### 2. Alert System

T√≠ch h·ª£p v·ªõi h·ªá th·ªëng c·∫£nh b√°o:

```python
def send_alert(row):
    if row['predicted_fraud'] == 1:
        # G·ª≠i email, SMS, webhook, etc.
        send_notification(
            f"FRAUD ALERT: TX {row['tx_id']} - "
            f"Amount: {row['amount']} - "
            f"Probability: {row['fraud_prob_pct']}%"
        )
```

### 3. Dashboard Real-time

S·ª≠ d·ª•ng `dashboard.py` ƒë·ªÉ visualize:
- S·ªë l∆∞·ª£ng giao d·ªãch theo th·ªùi gian
- T·ª∑ l·ªá gian l·∫≠n
- Accuracy metrics
- Top suspicious transactions

### 4. Model Retraining

ƒê·ªãnh k·ª≥ retrain model v·ªõi d·ªØ li·ªáu m·ªõi:

```bash
# Thu th·∫≠p d·ªØ li·ªáu m·ªõi t·ª´ Kafka/HDFS
# Ch·∫°y l·∫°i preprocessing.ipynb
# Ch·∫°y l·∫°i train.ipynb
# Model m·ªõi s·∫Ω t·ª± ƒë·ªông ƒë∆∞·ª£c load ·ªü l·∫ßn restart consumer ti·∫øp theo
```

## Best Practices

1. **Monitoring**: Lu√¥n theo d√µi metrics (accuracy, precision, recall)
2. **Threshold Tuning**: ƒêi·ªÅu ch·ªânh threshold theo business requirements
3. **Data Quality**: ƒê·∫£m b·∫£o d·ªØ li·ªáu input ƒë·∫ßy ƒë·ªß v√† ch√≠nh x√°c
4. **Model Update**: ƒê·ªãnh k·ª≥ retrain model v·ªõi d·ªØ li·ªáu m·ªõi
5. **Alerting**: Thi·∫øt l·∫≠p c·∫£nh b√°o cho fraud probability cao
6. **Logging**: L∆∞u l·∫°i predictions ƒë·ªÉ ph√¢n t√≠ch sau

## Performance

- **Throughput**: ~5-10 giao d·ªãch/gi√¢y (c√≥ th·ªÉ scale v·ªõi Spark cluster)
- **Latency**: ~100-500ms m·ªói prediction
- **Accuracy**: ~95-98% (t√πy thu·ªôc v√†o d·ªØ li·ªáu)

---

**L∆∞u √Ω**: H·ªá th·ªëng n√†y ch·ªâ mang t√≠nh demo. Trong production c·∫ßn:
- High availability setup (Kafka cluster, Spark cluster)
- Data backup v√† recovery
- Security (encryption, authentication)
- Comprehensive monitoring v√† alerting
