"""
Kafka Producer - Äá»c CSV tá»« HDFS vÃ  gá»­i vÃ o Kafka (giáº£ láº­p realtime)
"""

import time
import json
from kafka import KafkaProducer
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
import logging

# Cáº¥u hÃ¬nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class TransactionProducer:
    def __init__(self, kafka_broker='kafka:9092', topic='transactions'):
        """
        Khá»Ÿi táº¡o Kafka Producer
        
        Args:
            kafka_broker: Kafka broker address
            topic: Kafka topic Ä‘á»ƒ gá»­i dá»¯ liá»‡u
        """
        self.topic = topic
        
        # Khá»Ÿi táº¡o Kafka Producer
        self.producer = KafkaProducer(
            bootstrap_servers=kafka_broker,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            acks='all',
            retries=3,
            max_in_flight_requests_per_connection=1
        )
        
        logger.info(f"âœ… Kafka Producer Ä‘Ã£ káº¿t ná»‘i tá»›i {kafka_broker}")
        logger.info(f"ğŸ“¤ Sáº½ gá»­i dá»¯ liá»‡u vÃ o topic: {topic}")
        
    def send_transaction(self, transaction_dict):
        """Gá»­i 1 transaction vÃ o Kafka"""
        try:
            future = self.producer.send(self.topic, value=transaction_dict)
            # Block cho Ä‘áº¿n khi gá»­i thÃ nh cÃ´ng
            record_metadata = future.get(timeout=10)
            return True
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi gá»­i transaction: {e}")
            return False
    
    def close(self):
        """ÄÃ³ng producer"""
        self.producer.flush()
        self.producer.close()
        logger.info("ğŸ›‘ ÄÃ£ Ä‘Ã³ng Kafka Producer")


def read_csv_from_hdfs(hdfs_path, spark):
    """
    Äá»c CSV tá»« HDFS
    
    Args:
        hdfs_path: ÄÆ°á»ng dáº«n file CSV trÃªn HDFS
        spark: SparkSession
    
    Returns:
        DataFrame
    """
    logger.info(f"ğŸ“‚ Äá»c dá»¯ liá»‡u tá»« HDFS: {hdfs_path}")
    
    df = spark.read.csv(
        hdfs_path,
        header=True,
        inferSchema=True
    )
    
    total_records = df.count()
    logger.info(f"âœ… ÄÃ£ Ä‘á»c {total_records:,} records tá»« HDFS")
    
    return df


def simulate_realtime_streaming(df_spark, producer, delay_seconds=1):
    """
    Giáº£ láº­p streaming realtime báº±ng cÃ¡ch gá»­i tá»«ng record vÃ o Kafka
    
    Args:
        df_spark: Spark DataFrame
        producer: TransactionProducer instance
        delay_seconds: Thá»i gian chá» giá»¯a cÃ¡c láº§n gá»­i (giÃ¢y)
    """
    logger.info("="*80)
    logger.info("ğŸš€ Báº®T Äáº¦U STREAMING REALTIME")
    logger.info("="*80)
    logger.info(f"â±ï¸  Delay giá»¯a cÃ¡c transactions: {delay_seconds} giÃ¢y")
    logger.info("")
    
    # Chuyá»ƒn sang Pandas Ä‘á»ƒ dá»… iterate
    df_pandas = df_spark.toPandas()
    total = len(df_pandas)
    
    success_count = 0
    fail_count = 0
    
    try:
        for idx, row in df_pandas.iterrows():
            # Chuyá»ƒn row sang dictionary
            transaction = row.to_dict()
            
            # Convert numpy types sang Python native types
            transaction = {k: (int(v) if hasattr(v, 'item') and 'int' in str(type(v)) 
                              else float(v) if hasattr(v, 'item') and 'float' in str(type(v))
                              else v) 
                          for k, v in transaction.items()}
            
            # Gá»­i vÃ o Kafka
            if producer.send_transaction(transaction):
                success_count += 1
                
                # Log má»—i 100 records
                if (idx + 1) % 100 == 0:
                    logger.info(f"ğŸ“Š Progress: {idx+1}/{total} ({(idx+1)/total*100:.1f}%) - "
                              f"Success: {success_count}, Failed: {fail_count}")
                
                # Log chi tiáº¿t cho má»™t sá»‘ record Ä‘áº§u
                if idx < 5:
                    is_fraud = "ğŸš¨ FRAUD" if transaction.get('isFraud', 0) == 1 else "âœ… Normal"
                    logger.info(f"   â””â”€> Transaction #{idx+1}: Amount=${transaction.get('amount', 0):,.2f} - {is_fraud}")
            else:
                fail_count += 1
            
            # Delay Ä‘á»ƒ giáº£ láº­p realtime
            time.sleep(delay_seconds)
            
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  Nháº­n Ctrl+C - Dá»«ng streaming...")
    
    logger.info("")
    logger.info("="*80)
    logger.info("âœ… Káº¾T THÃšC STREAMING")
    logger.info("="*80)
    logger.info(f"ğŸ“Š Tá»•ng sá»‘ records Ä‘Ã£ gá»­i: {success_count}/{total}")
    logger.info(f"âŒ Sá»‘ records tháº¥t báº¡i: {fail_count}")
    logger.info("")


def main():
    """Main function"""
    # Cáº¥u hÃ¬nh
    HDFS_PATH = "hdfs://namenode:9000/data/input/paysim_realtime.csv"
    KAFKA_BROKER = "kafka:9092"
    KAFKA_TOPIC = "transactions"
    DELAY_SECONDS = 0.1  # Delay giá»¯a cÃ¡c transactions (giÃ¢y)
    
    # Khá»Ÿi táº¡o Spark Session
    logger.info("ğŸ”§ Khá»Ÿi táº¡o Spark Session...")
    spark = SparkSession.builder \
        .appName("Kafka Producer - Transaction Streaming") \
        .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    logger.info(f"âœ… Spark Session Ä‘Ã£ khá»Ÿi táº¡o (version: {spark.version})")
    
    try:
        # Äá»c dá»¯ liá»‡u tá»« HDFS
        df = read_csv_from_hdfs(HDFS_PATH, spark)
        
        # Hiá»ƒn thá»‹ schema
        logger.info("\nğŸ“‹ Schema cá»§a dá»¯ liá»‡u:")
        df.printSchema()
        
        # Hiá»ƒn thá»‹ máº«u
        logger.info("\nğŸ“Š Máº«u dá»¯ liá»‡u:")
        df.show(5, truncate=False)
        
        # Khá»Ÿi táº¡o Kafka Producer
        producer = TransactionProducer(
            kafka_broker=KAFKA_BROKER,
            topic=KAFKA_TOPIC
        )
        
        # Báº¯t Ä‘áº§u streaming
        simulate_realtime_streaming(df, producer, delay_seconds=DELAY_SECONDS)
        
        # ÄÃ³ng producer
        producer.close()
        
    except Exception as e:
        logger.error(f"âŒ Lá»—i: {e}", exc_info=True)
    
    finally:
        # Dá»«ng Spark
        spark.stop()
        logger.info("ğŸ›‘ ÄÃ£ dá»«ng Spark Session")


if __name__ == "__main__":
    main()
